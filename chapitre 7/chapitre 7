
# Quatre stratégies alternatives pour obtenir des données

Introduction 

### Recourir au droit d'accès
        
        


Malgré les limitations que nous avons exposés dans le chapitre 4, le recours au droit d’accès reste la première voie pour obtenir des données lorsqu’elles ne sont pas disponibles. Bien sur, le droit d’accès se limite uniquement aux données produites dans le cadre de missions de service public, exclut les secrets protégés comme celui des affaires ou des délibérations du gouvernement et ne permet pas de demander à l’administration de produire des données qu’elle ne détient pas. Mais, dans bien des cas, le droit d’accès s’est révélé être un contre-pouvoir efficace afin de permettre de disposer des informations dont les citoyens ont besoin. Aux Etats-Unis, le droit d’accès a permis l’émergence d’un genre journalistique à part entière, le *FOIA Journalism* qui consiste à s’appuyer quasi uniquement sur le droit d’accès pour réaliser une enquête. Récemment, le Washington Post a publié des révélations compromettantes avec les *Afghanistan Papers* en obtenant par le droit d’accès des documents révélant le degré d’impréparation de l’armée américaine à la guerre en Afghanistan. 

            

L’exercice du droit d’accès peut paraitre complexe mais il se relève très simple dans son application. 

            > **Comment faire une demande en vertu du droit d'accès ?**
            
            >L’exercice du droit d’accès est simple et demande peu de formalisme. La demande peut-être transmise par courriel ou auprès d’un formulaire de contacts, le site [madada.fr](http://madada.fr) vous permet de transmettre directement la demande. Au cas où la demande a été transmise au mauvais interlocuteur ou à une administration qui ne détient pas le document demandé, la loi prévoit que l’agent doit la transmettre au bon destinataire. 

            

Voici un message type qui peut être adapté selon les particularités de la demande : 

            

*Madame, Monsieur,* 

            

*En application de la loi n° 78-575 du 17 juillet 1978 relative aux documents administratifs, je souhaite recevoir communication des documents administratifs suivants:* 

\[Documents demandés\]

            

*Je souhaite recevoir ces documents dans un format numérique, ouvert et réutilisable. Pour ce faire, veuillez m’indiquer leur adresse de téléchargement ou me les envoyer en pièce jointe.*

            

*Comme le livre III du code des relations entre le public et l’administration le prévoit lorsque le demandeur a mal identifié celui qui est susceptible de répondre à son souhait, je vous prie de bien vouloir transmettre ma demande au service qui détient les documents demandés si c’est le cas.*

            

*Veuillez agréer, Madame, Monsieur, l'expression de mes sentiments distingués.*

            
>

L’exercice du droit d’accès est d’autant plus simplifié qu’il existe désormais des plateformes qui facilitent son exercice et rendent visibles les demandes. WhatDoTheyKnow, la première et la plus connue d’entre elles, a été lancée en 2006 par l’association britannique MySociety afin de faciliter les demandes. Basée sur le logiciel libre Alaveteli, cette plateforme permet de contacter directement le bon interlocuteur dans les collectivités pour exercer le droit d’accès et rend visible l’intégralité de la correspondance relative à la demande[^fn1].  La plateforme a connu un grand succès puisque plus de 500 000 demandes ont été effectuées par ce canal. Aux Etats-Unis, le site Muckrock administré par une organisation à but non-lucratif reprend ces fonctionnalités mais intègre une dimension éditoriale. Des journalistes administrent le site et lancent régulièrement des enquêtes collaboratives qui s’appuient sur des requêtes en vertu du *Freedom of Information Act.* Les correspondances relatives à ces requêtes sont visibles en ligne ainsi que tous les documents obtenus qui sont ensuite exploités dans des enquêtes. Aujourd’hui, on trouve des portails de droits d’accès à l’information dans près d’une vingtaine de pays. La plupart de ces portails s’appuie sur le logiciel libre Alaveteli développé par MySociety.

            

En France, il a pendant très longtemps été difficile de déployer un tel portail qui facilite et rend visible les demandes d’accès aux documents administratifs. Cela s’explique en partie par la difficulté de trouver les bons interlocuteurs dans les institutions publiques. Le Code des Relations entre le Public et l’Administration (CRPA) qui codifie entre autres la loi CADA prévoit que chaque administration désigne une PRADA (Personne Responsable de l’Accès aux Documents Administratifs) chargée de traiter les demandes d’accès. Mais le diable se cachant souvent dans les détails, il s’avère que la base des PRADA est rarement mise à jour, que de nombreuses collectivités n’en ont pas désigné mais que surtout cette base n’est pas disponible en open data. Un annuaire est bien disponible sur le site de la CADA⁠ mais il n’indiquait que les adresses postales alors que la loi permet la saisine par courriel. Avec la journaliste du *Monde* Laura Mottet, nous avons transmis une demande à la CADA afin d’obtenir les adresses mail des PRADA. Après un refus implicite de la CADA, nous avons saisi la même institution d’une demande d’avis (sic) sur notre requête à la CADA. L’institution a finalement validé notre demande dans son avis et nous a permis d’obtenir les données demandées. Avec ces données et une petite équipe de contributeurs bénévoles au sein de l’association Open Knowledge France[^fn2], nous avons pu lancer en octobre 2019 le portail [madada.fr](http://madada.fr) comme Ma Demande d’Accès aux Documents Administratifs. Au moment où j’écris ces lignes, le site a reçu une quarantaine de demandes en deux mois et permet de saisir près de 50 000 administrations publiques. Cette plateforme a déjà obtenu des résultats que ce soit pour identifier les données relatives à 7 milliards d’euros de subvention attribuées par l’Etat aux associations[^fn3] ou obtenir le nombre de véhicules passant sur les 5 dernières années pour plus de 200 capteurs en région PACA[^fn4]. La visibilité des échanges sur la plateforme devrait favoriser une plus grande réactivité des administrations en exposant sur la place publique une procédure qui se déroulait jusqu’alors en coulisses. Ces réussites ne doivent toutefois pas cacher les difficultés récurrentes rencontrées par les demandeurs, qu’ils passent par une plateforme comme [madada.fr](http://madada.fr) ou traitent en direct avec les administrations. Même si la base des PRADA nous indique les contacts privilégiés dans les administrations, de nombreuses demandes n’aboutissent pas du fait qu’elles n’arrivent pas aux bonnes personnes ou que les destinataires ne connaissent pas les règles du droit d’accès. Si la demande est bien reçue, obtenir un accusé de réception est rare même si la loi le prévoit et le silence est courant, beaucoup d’administrations en particulier locales attendant un avis de la CADA avant de se prononcer même pour des demandes ne faisant aucunement débat. Enfin, lorsqu’on obtient les données, ces dernières ne correspondent souvent pas aux exigences de la loi pour une République numérique en matière de lisibilité par les machines et d’ouverture des formats. Plusieurs demandes sur [madada.fr](http://madada.fr) ont abouti à un courrier indiquant comme télécharger un fichier PDF de subventions ou renvoyant des graphiques dans un rapport dans lequel il est impossible d’exporter les données. 

            

Malgré les limites connues du droit d’accès, ces plateformes ouvrent la possibilité inédite de constituer des bases de données à une échelle nationale à partir de demandes multiples. Une des limitations de l’ouverture des données locales réside dans le fait que les données publiées par une collectivité ne sont pas toujours ouvertes dans les autres territoires déjà investis dans une démarche d’open data. Rappelons par ailleurs que ces collectivités sont encore une exception. Selon les données de l’observatoire open data des territoire d’Open Data France[^fn5], seulement 7,8% des 5101 collectivités concernées par la loi pour une République numérique ont effectivement ouvert des données. Parmi celles-ci, 31% ont ouvert seulement un jeu de données et 67,5% au maximum 10 jeux de données. Il est donc rare que, pour un même jeu de données, on arrive à retrouver le même type de données dans un nombre suffisant de collectivités pour atteindre une couverture complète du territoire. Dans ce contexte, ces plateformes offrent la possibilité de réclamer le même jeu de données dans un grand nombre de territoire afin de créer une base de données à l’échelle nationale de phénomènes dont on ne dispose que de données produites par des collectivités locales. A cette fin, MySociety vient de développer une fonctionnalité pour WhatDoTheyKnow afin de saisir à la volée jusqu’à plusieurs milliers d’administrations et d’assurer le suivi de ces demandes. En France, la société [VroomVroom.fr](http://VroomVroom.fr) est née suite à ce modèle de demandes CADA à la volée. Le site qui propose un comparateur des auto-écoles en France a demandé à chaque préfecture de département les statistiques de performance de chaque auto-école et les a compilé dans une base de données unique qui a servi de base au développement du site.

            

Le droit d’accès, même simplifié et rendu visible, reste limité par l’étape du recours auprès du tribunal administratif qui réclame des compétences juridiques rares et n’est pas a priori accessible au citoyen en quelques clics. Rappelons que les avis de CADA sont consultatifs et qu’une administration peut toujours refuser la communication d’un document administratif suite à un avis favorable. Seul un jugement du tribunal administratif peut l’y contraindre. Bien que la démarche soit gratuite, le recours devant le juge administratif demande des compétences juridiques avancées, notamment dans la rédaction d’un mémoire. L’association Ouvre Boite qui vise à « obtenir l'accès et la publication effective des documents administratifs » s’est fait la spécialité de tels recours. Elle met à disposition sur son site des exemples de mémoire au tribunal administratif pour contester un avis de la CADA ou obtenir la communication d’un document. Son action a permis l’ouverture de bases de données emblématiques comme le registre parcellaire graphique qui donne les contours des parcelles agricoles avec le type de culture, le répertoire des professionnels de santé, le code source de l’algorithme de calcul des aides de la CAF ou encore le code de l’algorithme du calcul de l’impôt sur le revenu. Ces exemples rappellent que le droit d’accès est un travail de long haleine qui, avec de la pratique et de la tenacité, peut s’avérer fructueux. Mais, dans bien des cas, les personnes qui y ont besoin de données ne font pas preuve d’une telle patience. Le scrapping ou l’extraction de données issues de sites web peut s’avérer être une stratégie payante

### Scrapper les données sur le web


Le scraping[^fn6] consiste à extraire les données présentes sur des sites web généralement dans des pages éparses. Il est courant qu’une organisation diffuse des données sur le web mais ne mette pas à disposition les données derrière ces sites. Or, dans beaucoup de cas sur les sites publics, ces données auraient vocation à être librement réutilisables par toutes et tous. Le outils de scraping permettent d’identifier les éléments à extraire sur les pages (généralement par leurs propriétés dans le code source de la page ou dans la feuille de style), d’identifier toutes les pages concernées et d’extraire les éléments dans une base de données structurée.

            > **Comment scraper des données sur le web sans savoir programmer ?**
            
            >Le scraping peut être pratiqué assez simplement sans savoir nécessairement coder. Plusieurs outils 

            

Webscraper

Scraper>

Lors de la conférence fondatrice des principes de l’open data à Sebastopol en 2007, un des participants, Joshua Tauberer, est le créateur de [GovTrack.us](http://GovTrack.us). Ce site de suivi de l’activité parlementaire est basé sur l’extraction automatique des données publiées sur le site du Congrès des Etats-Unis. Tauberer a joué un grand rôle dans la diffusion des principes de l’open data en publiant le site de références [opengovdata.org](http://opengovdata.org) et en écrivant le livre *Open Gov Data* \{Tauberer:2014td\}.  La présence de Tauberer à la conférence fondatrice et son activisme par la suite ne sont pas anodins. D’une certaine façon, les principes de l’open data ont tenté de résoudre les limitations du scraping qui s’apparente souvent à un bricolage. En l’absence de source de données ouvertes fiables, les scripts qui permettent le scraping sont sensibles à la moindre évolution des sites web. Une modification de la feuille de style ou de nouveaux éléments ajoutés sur le page rendent souvent le script caduque et les données sont alors corrompues. L’organisation à l’origine du site peut facilement interrompre le scraping, par exemple en changeant la structure du site, soit en bannissant les adresses IP correspondant aux scripts ou en empêchant un affichage trop rapide des pages. En demandant la mise à disposition des bases de données à la source dans un format lisible par les machines, les principes de l’open data tentent de pérenniser les outils réutilisant des données publiques là où le droit d’accès ou le scraping reposent sur des assemblages fragiles. 

            

En France, la création d’un site de suivi de l’activité parlementaire similaire à [GovTrack.us](http://GovTrack.us) a donné lieu à la création de l’association Regards Citoyens, une des principales organisations militant en faveur de l’open data dans le pays. En 2009, l’association a créé nosdeputes.fr puis [nossenateurs.fr](http://nossenateurs.fr) pour suivre l’activité des parlementaires : leurs amendements, leurs questions écrites ou orales, leurs interventions en hémicycle comme en commission, leurs rapports ou propositions de loi . Ces sites sont nés à partir du scraping des sites de l’Assemblée nationale et du Sénat où l’activité des parlementaires est éparpillée dans des documents HTML ou PDF qui sont convertis dans des bases de données[^fn7]            . Ces données permettent offre à chacun de nouveaux outils d’accès à ses élus : comparaison des activités des députés par groupe politique, représentation sur un graphe de l’activité individuelle des députés, nuages des mots-clés les plus employés par un député, comptes-rendus de séances enrichis de liens contextuels, alertes mails, possibilité de commenter chacun des travaux réalisés par les parlementaires... Bien que désormais l’Assemblée nationale et le Sénat proposent des portails open data, le scraping reste la principale source de données pour ces sites. Regards Citoyens doit donc régulièrement adapter ses scripts pour continuer à extraire les données malgré les évolutions régulières des sites parlementaires.  

            

La plupart des exemples de scraping qu’on pourra citer témoigne de la fragilité sur le moyen et long terme de cette technique d’obtention des données. Le journaliste Alexandre Léchenet avait réalisé en 2012 une enquête sur les dépassements d’honoraires des médecins (figure 5) pour *Le Monde* en extrayant les données du site [ameli-direct.fr](http://ameli-direct.fr) qui indique les tarifs habituellement exercés par chaque praticien. ![][CarteLeMonde]

*Figure 5. Carte des dépassements d’honoraires à Paris réalisée à partir des données d’*[*ameli-direct.fr*](http://ameli-direct.fr)

            

Ces données n’étaient pas mises à disposition par l’Assurance Maladie. Pour les extraire, il a fallu développer un robot qui devait contourner les différentes protections (cookie, moteur de recherche en Flash, URL unique pour chaque session...) mises en place sur le site[^fn8]. Quelques mois après la publication de la carte par *Le Monde* qui révélait qu’en moyenne à Paris les dépassements étaient de 15€ par consultation, l’Assurance Maladie a fait évoluer le site pour empêcher de nouvelles extractions. Lors d’une audition par la mission commune d’information du Sénat sur l’open data, Mathieu Escot, chargé de mission santé à UFC-Que Choisir, expliquait que l’association avait du extraire les données d’ameli-direct pour conduire une étude similaire sur les dépassements d’honoraires des cabinets médicaux. Selon lui, cette opération a coûté 20 000€ à l’association « soit l'équivalent du budget annuel du pôle santé de l'association pour l'achat de prestations extérieures[^fn9]. » Concrètement, ces différentes protections, qui souvent visent d’abord à protéger des données personnelles ou sensibles, restreignent les capacités d’action de la société civile aux associations ou médias les plus aisés. Dans un tout autre domaine, la Cimade, une association de défense des réfugiés et du droit d’asile, a mis en place le site « A guichets fermés » pour dénoncer l’attente pour les formalités d’immigration devenue « un moyen de domination des personnes étrangères par la préfecture. » L’association constate depuis plusieurs années que la dématérialisation des procédures crée des « files d’attente invisibles » et renforce la précarité des personnes immigrées. Pour produire le site « A guichets fermés », l’association a développé des robots qui se rendent toutes les deux heures sur les sites de prise de rendez-vous en préfecture et extrait le délai d’attente pour les principales formalités. Ces données ont permis de montrer que, dans de nombreux départements, les personnes en situation régulière ne peuvent jamais prendre de rendez-vous alors que le délai pour un visa de long séjour s’étend de un à deux mois dans la même préfecture. Le site propose pour chaque préfecture des visualisations du délai d’attente pour les différentes procédures (*figure \<$n:figure:agf)*, des graphiques qui peuvent être exportées à des fins de preuves dans des recours administratifs.  ![][Capturedécran2019-12-26à224417]

*Figure 6 Exemple de délais dans une préfecture sur le site « A Guichets Fermés » de la Cimade*

            

En l’absence de données officielles à ce niveau de précision, le scraping a permis de dénoncer des délais extrêmement longs, décourageant les réfugiés et les poussant à la clandestinité. Mais ces données n’ont pas permis de résorber la pénurie de rendez-vous, bien au contraire. En 2018, le ministère de l’Intérieur a voulu mettre un terme aux plateformes automatisées qui monétisaient les créneaux réservés, profitant de la rareté des rendez-vous pour créer un marché parallèle. De mai à juin 2018, le robot de la Cimade a été bloqué, en même temps que les autres outils services automatisés, jusqu’à ce que l’association trouve un contournement. En avril 2019, le robot a été encore interrompu « et contrairement aux \[blocages\] précédents, un faisceau d’éléments indique que ce blocage s’adresse spécifiquement à La Cimade, facilement identifiable par l’adresse IP du robot[^fn10]. » Deux semaines après avoir interpellé le ministre et publié un communiqué de presse, la Cimade a pu de nouveau collecter les délais d’attente en préfecture suite à une intervention de la Place Beauveau.  

            

Pour témoigner encore une fois de la fragilité du scraping, on peut citer l’exemple récent du décompte des soutiens à la proposition de loi s’opposant à la privatisation d’Aéroports de Paris (ADP). Le site du référendum d’initiative partagée (RIP) lancé par le ministère de l’Intérieur ne fournissait aucun compteur des signatures alors que la proposition doit être signée par 10% du corps électoral (soit 4,7 millions de citoyens). Pour le ministère, c’était la prérogative du Conseil constitutionnel. Quelques jours après le lancement du site, deux sites ont été lancés par des citoyens pour comptabiliser les soutiens en contournant les obstacles placés par le ministère sur le site du RIP. Afin d’extraire le nombre de soutiens présentés sur plusieurs milliers de pages rangées par ordre alphabétique, il fallait valider à chaque page un captcha qui visait à valider que l’utilisateur est un humain. Pour contourner le captcha, un premier site avait eu recours à des microtravailleurs rémunérés à la tache tandis que l’autre proposait d’installer une extension sur son navigateur qui extrayait page par page le nombre de signatures. Ces deux méthodes étant particulièrement dépendantes des contributions humaines, une faille a été trouvée sur le site du RIP avec une page qui recensait la totalité des pages de la liste de soutien. Dans la journée après la découverte de cette page, le ministère de l’Intérieur a bloqué cette page obligeant les compteurs à revenir à leur précédentes méthodes de comptage[^fn11]. Ces exemples montrent que le scraping peut s’avérer être une méthode efficace pour obtenir rapidement des données de valeur mais qu’elle s’avère incertaine sur la durée sans procéder à des ajustements incessants. 

            


### Constituer des données à partir des informations disponibles

Ecriture en cours : 
        
* Etymologie de données : datum, ce qui est admis. Comme l’a expliqué Rosenberg, le terme datum est encore utilisé dans certaines communautés scientifiques pour désigner un point de données, une cellule dans un tableur ou une valeur dans une base de données. Datum est un singulier mais ce qui fait la valeur des données vient de leur aggrégation, de la somme des datum. Prise en tant que tel, une datum n’a que peu de valeur, c’est son accumulation qui constitue la valeur des données, d’où l’apparition progressive du pluriel data au mot latin.

* Comment faire quand on ne dispose uniquement que de chiffres épars et pas d’une base de données ? Comment passer de datum à des données ? 

* La troisième stratégie pour obtenir des données lorsqu’elles ne sont pas disponibles consiste à constituer des données à partir des informations disponibles. Rappelons le, on distingue classiquement les données qui sont lisibles par les machines de l’information qui est intelligible par les humains (Ackoff, Kitchin, Denis & Goeta). Selon cette définition, l’information sera la synthèse des données sous une forme intelligible par les humains. Avant le développement de l’open data, beaucoup d’informations étaient disponibles sous la forme de rapports ou de documents PDF. Ce format a pour particularité de rester immuable et de ne pas permettre à l’usager de le transformer par exemple pour extraire les données qui y figurent (Gitelman). Ces rapports comportent généralement des graphiques, des tableaux ou des indicateurs qui pris isolément ne permettent pas de refaire les calculs et de produire de nouvelles analyse. Au contraire, l’ouverture des données, lorsque celles-ci sont granulaires et d’un haut niveau de précision, facilite le développement de nouvelles analyses. 

* Bien que l’ouverture des données soit en France une obligation légale pour les acteurs investis d’une mission de service public, il est encore courant que des administrations publient des documents PDF dont il est très difficile d’extraire les données. 

* Par exemple, l’association Regards Citoyens a publié en juillet 2014 un site pour numériser les déclarations d’intérêts des parlementaires. Suite à la loi du 11 octobre 2013 sur la transparence de la vie publique, les parlementaires doivent déclarer leurs intérêts à la Haute Autorité pour la Transparence de la Vie Publique en charge de les contrôler et de les rendre publics afin que chaque citoyen puisse évaluer les possibles risques de confilts d'intérêts de ses représentants. La loi prévoit que les déclarations d'intérêts soient diffusées en open data mais, ayant pris ses fonctions en décembre 2013 et devant collecter l’ensemble des déclarations avant janvier 2014, la HATVP a préféré garder l’ancienne procédure de la commission pour la transparence qui nécessitait de remplir un formulaire papier plutôt que de remplir un formulaire en ligne (Déclarations d’intérêts des politiques : La HATVP vise un téléservice dès 2015---Next INpact. (s. d.). Consulté 28 juillet 2014, à l’adresse <http://www.nextinpact.com/news/88901-declarations-d-interets-politiques-hatvp-vise-teleservice-des-2015.htm->) En juillet 2014, la HATVP a publié un jeu de données recensant les déclarations d'intérêts mais ces dernières étaient diffusées sous la forme de PDF images scannant les formulaires généralement manuscrits des parlementaires. Il était alors quasiment impossible d’extraire le texte manuscrit, indéchiffrable par la plupart des outils de reconnaissance de caractères (OCR). Regards Citoyens a ainsi développé une plateforme collaborative afin d’inviter les citoyens à numériser ces informations essentielles à la vie publique. Le site affichait d’un côté l’extrait de la déclaration du parlementaire et de l’autre une interface pour numériser chacun des éléments contenues dans la section du formulaire (figure 7). 

![][Capturedécran2019-12-28à110711]

Figure 7. Capture d’écran du site « Numérisons les intérêts des parlementaires » développé par Regards Citoyens (Regards Citoyens. (2014). Numérisons les déclarations d’intérêts. Consulté 28 juillet 2014, à l’adresse Numérisons les déclarations d’intérêts---Regards Citoyens website: <http://www.regardscitoyens.org/interets-des-elus/declaration.php?partie=12&nom=Jacques%20Myard>) 

        

Le travail était rendu particulièrement difficile par le fait que certaines déclarations étaient pratiquement illisibles du fait de l’écriture manuscrite de son auteur, de ratures, d’annotations ou parfois même de déclarations hostiles à la démarche de transparence. Pour éviter d'intégrer toute erreur de saisie ou tentative de vandalisme, chaque extrait de formulaire était présenté au hasard aux utilisateurs et n'est considéré comme valablement numérisé que lorsque 3 utilisateurs différents auront saisi les mêmes informations. Certaines déclarations ont nécessité l’intervention de près de 70 personnes différentes pour être numérisées comme celle de Xavier Bertrand (figure 8). 

![][bertrand-xavier-dia-depute-02-mask-5_1]

Figure 8. Extrait de la déclaration d’intérêt de Xavier Bertrand (Regards Citoyens. (2014). 8000 personnes libèrent en une semaine les données manuscrites des déclarations d’intérêts des parlementaires ! Consulté 4 août 2014, à l’adresse <http://www.regardscitoyens.org/8000-personnes-liberent-en-une-semaine-les-donnees-manuscrites-des-declarations-dinterets-des-parlementaires/>)

Au final, les déclarations remplies des 577 députés et 348 sénateurs comportant chacune 12 parties, soit un total de plus de 11 000 extraits de formulaires manuscrits, ont été numérisées en une semaine par près de 8000 contributeurs. Les données ont été republiées sous la forme d’un fichier csv comprenant tous les éléments contenues dans les déclarations permettant par exemple une analyse du nombre de collaborateurs par parlementaire. Depuis, la HATVP a généralisé la télédéclaration des intérêts pour les personnes concernées et diffuse en open data l’intégralité des déclarations collectées (Les déclarations des responsables publics désormais accessibles en open data « Haute Autorité pour la transparence de la vie publique. (s. d.). Consulté 25 septembre 2017, à l’adresse <http://www.hatvp.fr/presse/les-declarations-des-responsables-publics-desormais-accessibles-en-open-data/>).

        

* Dans ces deux cas, le travail de numérisation sous la forme de données lisibles par les machines a été grandement facilité par le fait que les documents étaient disponibles en un seul et même endroit. Mais il est courant que les sources soient éparpillées sur le web ; au travail de numérisation, s’ajoute alors un effort de collecte qui rend la tâche particulièrement ardue. En 2015, j’avais travaillé avec  le Syndicat de la Presse Indépendante d’Information en Ligne (SPIIL) dans une action de plaidoyer en faveur d’une meilleure répartition des aides à la presse pour les acteurs de la presse en ligne. Depuis les Etats généraux de la presse de 2008, le syndicat constate que les réformes s’enchainent pour accompagner le basculement vers le numériques des entreprises de presse alors que les usages ont déjà considérablement transformé. Mais, dans les chiffres, les adhérents du SPIIL ont constaté que les aides étaient toujours en immense majorité réservées au support papier. Pour objectiver ce constat, le SPIIL a du compiler l’ensemble des informations pour constituer une base de données formant un panorama complet des aides à la presse. A l’époque, les données disponibles ne concernaient pas l’ensemble des titres (seulement les 200 titres les plus aidés) ni l’ensemble des dispositifs (uniquement certaines aides directes). Pour disposer d’une vue d’ensemble des aides à la presse, le SPIIL a constitué une base de données estimant le montant de chacun des dispositifs d’aide à partir de sources très diverses (figure 9): rapports annuels de performance établis par Bercy, rapport de la Cour des Comptes, budget de l’Etat, questions au gouvernement ou encore des estimations publiées par le SPIIL...

        

![][Capturedécran2020-01-03à115912]

*Figure 9. Evolution annuelle des montants des différents dispositifs d’aides à la presse estimée par le SPIIL à partir de sources variées.* 

         

Avec ces données constituées par le SPIIL, j’ai accompagné le syndicat dans la visualisation de ces données avec des outils simples d’utilisation. Chaque visualisation a été conçue pour illustrer les messages portés par le SPIIL dans l’analyse de ces données (figure 10.)  

![][spiilviz]

*Figure 10. Visualisation de données produites avec le SPIIL sur l’évolution des aides à la presse*

        

Ce cas illustre une difficulté courante dans la constitution d’une base de données à partir des informations disponibles sur le web. Les sources sont éparses et seul un travail manuel imposant suffit à constituer une base de données à partir d’informations jamais structurées de la même manière dans les différentes publications ou rapports qui assurent depuis longtemps la transparence étatique. J’ai aussi rencontré cette épreuve du rassemblement de sources éparses lors d’une campagne de mobilisation que Datactivist avait conçu avec Greenpeace. En préparation des États Généraux de l’Alimentation, Greenpeace souhait montrer que nos enfants mangent au moins deux fois trop de viande dans les cantines scolaires mais ce constat cache une situation très hétérogène selon les villes. Certaines ont fait des efforts notables, d’autres continuent à servir viande et poisson tous les jours. Pour objectiver ce constat, il n’existe aucune base de données au niveau national sur la composition des menus. Il y a bien quelques villes en France qui ont ouvert des données sur les menus des cantines comme Toulouse qui propose les menus quotidiens, les plats qui les composent et les denrées qui servent à leur élaboration. Pour obtenir les données dont nous avions besoin, il nous a donc fallu concevoir une enquête contributive dans laquelle la communauté de Greenpeace pourraient remonter les données concernant leurs écoles via une plateforme en ligne. 

        

        

        

        

Lancée début décembre, les résultats et les données de la grande enquête “Y a-t-il trop de viande à la cantoche ?” viennent d’être publiés (voir plus bas). Elle a permis de montrer que 70% des enfants mangent viande ou poisson tous les jours et de décrire la situation dans plus de 3200 villes.

        

Greenpeace menus des cantines

        

        

        

        

        

        

        

Radars

SPIIL 

        

Carte de la corruption

Migrant Files

        

Just Good Enough Data 


        ### Créer une base de données collaborative ###

[CarteLeMonde]: CarteLeMonde.png width=278px height=266px

[Capturedécran2019-12-26à224417]: Capturedécran2019-12-26à224417.png width=322px height=219px

[Capturedécran2019-12-28à110711]: Capturedécran2019-12-28à110711.png width=1278px height=617px

[bertrand-xavier-dia-depute-02-mask-5_1]: bertrand-xavier-dia-depute-02-mask-5_1.jpg width=240px height=136px

[Capturedécran2020-01-03à115912]: Capturedécran2020-01-03à115912.png width=930px height=412px

[spiilviz]: spiilviz.png width=676px height=310px

[^fn1]: Les demandes relatives à des données personnelles ne sont pas possible sur le portail du fait que les données obtenues se retrouveraient sur la place publique.

[^fn2]: Je profite de ce livre pour les citer et les remercier : Laurent Savaete, Pierre Chrzanowski, Pascal Romain, Anne-Laure Fréant, Thomas Parisot. La contribution de Claude Archer de transparencia.be a aussi été déterminante pour le lancement du site.

[^fn3]: [ https://madada.fr/demande/subventions_aux_associations_oct#comment-4](https://madada.fr/demande/subventions\_aux\_associations\_oct#comment-4)

[^fn4]: [ https://madada.fr/demande/donnees_trafic_du_reseau_de_la_d](https://madada.fr/demande/donnees\_trafic\_du\_reseau\_de\_la\_d)

[^fn5]: [ http://www.observatoire-opendata.fr/resultats/](http://www.observatoire-opendata.fr/resultats/)

[^fn6]: Ce terme anglais vient du verbe *scrape* qui signifie toucher à la surface et  en informatique désigne « copier des données d’un site web. » Il n’y a pas à ma connaissance de traduction satisfaisante et la plupart des acteurs emploient ce terme sans le traduire.

[^fn7]: Pour en savoir plus, lire l’article « Comment Regards Citoyens a créé NosDéputés.fr, base de données de l’activité parlementaire » dans \{Gray:2013ul\}. Accessible en ligne à l’adresse suivante : http://jplusplus.github.io/guide-du-datajournalisme/pages/0417.html

[^fn8]: Pour en savoir plus, lire l’article « Aspirer les données d’Ameli » dans \{Gray:2013ul\}. Accessible en ligne à l’adresse suivante :http://jplusplus.github.io/guide-du-datajournalisme/pages/0403.html

[^fn9]: [ http://www.senat.fr/compte-rendu-commissions/20140113/mci_cada.html](http://www.senat.fr/compte-rendu-commissions/20140113/mci\_cada.html)

[^fn10]: https://www.lacimade.org/dematerialisation-service-public-personnes-etrangeres-ministere-interieur-contre-transparence/?fbclid=IwAR3aFUdDgZvA-HeR8cPAUBQCoDUh7PnmPwcqITKcNYdX3iJtuXaj7fgp754

[^fn11]: [ https://www.liberation.fr/checknews/2019/06/26/referendum-adp-l-interieur-casse-le-compteur_1736096](https://www.liberation.fr/checknews/2019/06/26/referendum-adp-l-interieur-casse-le-compteur\_1736096)